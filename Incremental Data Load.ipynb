#Load Data into Delta Lake 

initial_data = [
    (1, "Alice", "2025-07-20"),
    (2, "Bob", "2025-07-20"),
    (3, "Charlie", "2025-07-20")
]
df_initial = spark.createDataFrame(initial_data, ["id", "name", "updated_date"])

# Write as Delta table with CDF enabled
(df_initial.write
 .format("delta")
 .mode("overwrite")
 .option("overwriteSchema", "true")
 .saveAsTable("mock_source_table"))

spark.sql("
ALTER TABLE mock_source_table 
SET TBLPROPERTIES (delta.enableChangeDataFeed = true)")

####Update Data 

%sql
update mock_source_table
set name = 'Alicia'
where name = 'Alice';

%sql
INSERT INTO mock_source_table (id, name,updated_date)
VALUES (4, "David", "2025-07-22");

%sql
DELETE FROM mock_source_table WHERE id = 2;




#create delta version control table
data = [("mock_source_table", 0, "2025-07-22")]
ct_df = spark.createDataFrame(
    data, ["table_name", "last_processed_version", "last_processed_date"]
)
# Write as Delta table
(ct_df.write
 .format("delta")
 .mode("overwrite")
 .option("overwriteSchema", "true")
 .saveAsTable("delta_versions_control_table"))

# df_initial.write.format("delta").mode("overwrite").saveAsTable("mock_target_table")

from delta.tables import DeltaTable
target_delta = DeltaTable.forName(spark, "mock_target_table")

# Apply incremental changes from CDF
(
    target_delta.alias("t")
    .merge(cdf_df.alias("s"), "t.id = s.id")
    .whenMatchedUpdateAll(condition="s._change_type = 'update_postimage'")
    .whenMatchedDelete(condition="s._change_type = 'delete'")
    .whenNotMatchedInsertAll(condition="s._change_type = 'insert'")
    .execute()
)
